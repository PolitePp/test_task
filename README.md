# Тестовое задание в компанию, чьё имя нельзя называть
Тестовое задание состоит из двух частей. В первом задании необходимо написать запросы к БД. Во втором задании необходимо написать процесс реконсиляции данных.

## Приступая к работе

Инструкция о том, как получить копию этого ПО и запустить его на локальном компьютере с целью разработки и тестирования. Подробную информацию о развертывании ПО в условиях эксплуатации см. в разделе «Развертывание».

### Предварительные условия

Что нужно для установки ПО, инструкции по установке дополнительных компонентов.

```
Docker
Docker-compose
Git
Unzip
DBeaver
```
Протестировано на ОС Ubuntu 18.04.5 LTS, docker 20.10.5, docker-compose 1.28.4, git 2.17.1

### Установка

В первую очередь загрудите данный репозиторий следующей командой:

```
git clone https://github.com/PolitePp/test_task.git
```

Перейдите в корневую директорию проекта

```
cd tesk_task/
```

Запустите run.sh скрипт командой

```
sh run.sh
```

В данном bash скрипте указаны команды по поднятию контейнеров, разархивированию json файла, выполнения restore для postgres, а так же создание директории в hdfs и загрузки туда json файла

# Проверка заданий
## Задание 1
Всё, что связано с заданием 1 расположено в папке **first_task**

### Структура папки first_task

    .
    ├── input                         # Пробрасывается на контейнер субд Vertica. Входные данные
    │   ├── queries.txt               # Файл, содержащий запросы, для которых необходимо оптимизировать выполнение
    ├── output                        # Пробрасывается на контейнер субд Vertica. Результат работы Database Designer
    │   ├── my_design_deploy.sql      # Вызывается для запуска my_design_projections.sql
    │   ├── my_design_projections.sql # Вызывается для запуска my_design_projections.sql
    ├── sql_scripts                   # Скрипты на создание, наполнение и анализ
    │   ├── create_structures.sql     # Скрипт с созданием таблиц
    │   ├── insert_data.sql           # Скрипт с наполнением таблиц
    │   ├── task_1_2.sql              # Скрипт с заданием 1.2
    │   ├── task_1_3.sql              # Скрипт с заданием 1.3
    │   ├── task_1_4.sql              # Скрипт с заданием 1.4

### Комментарии по реализации и выбору технологий
Была выбрана СУБД Vertica по нескольким причинам, а именно:
1. СУБД, с которой работает компания
2. MPP, следовательно, можно будет параллелить запросы
3. Колоночное хранение хорошо подходит для аналитических запросов

Информация по подключению:
```
user dbadmin
host localhost
port 5433
database docker
Пароль не требуется
```

ER модель:
https://app.diagrams.net/#G1JDFThcYv8NfXtzmIghuoCRFC2Dbb_ZEc

Очерёдность выполнения скриптов:
```
create_structures.sql
insert_data.sql
```

После выполнения insert_data.sql необходимо зайти внутрь контейнера vertica_db для деплоя projections

```
docker exec -it vertica_db bash
/opt/vertica/bin/vsql -h localhost --dbname docker --user dbadmin
\i /tmp/output/my_design_deploy.sql
```

И затем можно смотреть запросы, которые находятся в файлах:
```
task_1_2.sql
task_1_3.sql
task_1_4.sql
```

Комментарии:
+ Создавал ПК и ФК, не уверен, насколько это актуально для DWH
+ Наполнял с помощью функционала, который был в СУБД
+ Не делал автоматическое наполненеи и создание таблиц через sh, чтобы можно было ознакомиться с реализацией наполнения. Т.к. работал с Vertica в первый раз, такое наполнение показалось жизнеспособным, хотя и, на некоторых вставках, медленное.  
+ По-сути, первый запрос можно было написать и без CTE. Т.е. сразу сделать все необходимые джойны и группировать после. Но тогда кол-во строк очень сильно бы размножилось. Т.к. сначала соединяемся с операциями, а потом ещё со сделками.

Более подробные комментарии указаны в самих запросах

## Задание 2
Всё, что связано с заданием 2 расположено в папке **second_task**

### Структура папки second_task

    .
    ├── input_hdfs                    # Пробрасывается на контейнер namenode
    │   ├── transactions.zip          # Архив, содержащий json файл с транзакциями
    ├── pg_dump_archive               # Пробрасывается на контейнер postgresql
    │   ├── create_structures.sql     # Скрипт с созданием таблиц
    ├── spark_jobs                    # Пробрасывается на контейнер spark-master для выполнения джобы
    │   ├── config.ini                # Конфигурационный файл, содержащий информацию по подключению к БД и hdfs
    │   ├── postgresql-42.2.19.jar    # Драйвер postgres для подключения к СУБД PostgreSQL
    │   ├── reconciliation_job.py     # Джоба для спарка


### Комментарии по реализации и выбору технологий
Было принято решение использовать PySpark, по следующим причинам:
1. Позволяет масштабировать выполнение джобов и примерим к BigData
2. Позволяет писать джобы с использованием Python
3. Позволяет получать информацию из различных источников данных

Комментарии по данным:
1. Первым источником данных является - PostgreSQL. Именно там я нагенерировал данные и потом перенёс в json, который поправил, чтобы были "ошибки"
2. Вторым источником данных является json, который лежит в hdfs (для того, чтобы не раскидывать файл на все ноды)

Комментарий по джобе:
1. При написании использовался linter Pylint
2. На вход джобе можно подать параметр с толеранс. Есть небольшая проверка на то, что значение можно кастануть к float и что значение находится в промежутке от 0 до 100. Если было подано ошибочное значение (или не подано вообще), то значение по умолчанию - 0
3. Реконсиляция сделана с учётом того, что структура данных (первичный ключ, список колонок) известны заранее
4. Реконсиляция проводится для указанных типов данных, а именно дата, текст, числа
5. Учитывается значение толеранса
6. coalesce(1) используется для того, чтобы на выходе получить один файл, который удобно скачать. Один из вариантов, можно убрать coalesce и объединять файлы на стороне hdfs.
7. В случае, если у нас есть строка в одном источнике, но отсутствует в другом, то при full join для ключа, который мы не нашли, везде будет Null. Я решил, что их необходимо учитывать при сравнении полей. В случае, если это было не нужно, то при создании колонок для сравнения необходимо фильтровать строки, в которых один из ключей is Null
8. В самой джобе комментарии на английском для того, чтобы продемонстрировать его знание :). В принципе, комментариев не так много, т.к. старался называть всё со смыслом.

На выходе получаем файлы, хранящиеся в hdfs. Посмотреть и скачать их можно перейдя в браузере по следующей ссылке:
http://localhost:9870/explorer.html#/

В корневой директории будет 3 папки, а именно:

    .
    ├── raw                    # Здесь лежит исходный json
    │   ├── transactions.json       # json с транзакциями
    ├── reports                # Здесь лежит отчёт по результатам реконсиляции
    │   ├── report.csv              # Отчёт со статистикой. Внутри лежит *.csv файл
    ├── errors                 # Здесь лежат csv файлы в зависимости от типа ошибки
    │   ├── db_rows.csv                      # Строки, которым не было найдено соответствии в json файле
    │   ├── json_rows.csv                    # Строки, которым не было найдено соответствии в БД
    │   ├── amount_mismatch.csv              # Расхождения по полю amount
    │   ├── client_account_mismatch.csv      # Расхождения по полю client_account
    │   ├── transaction_date_mismatch.csv    # Расхождения по полю transaction_date
    │   ├── transaction_type_mismatch.csv    # Расхождения по полю transaction_type


Отчёт со статистикой в формате CSV. Пример:

| field_name | matching_record_count | mismatch_record_count | matching_record_percentage |
| ------ | ----------- | ------ | ----------- |
| matching_record_count | 999995 | 0 | 0.0 |
| mismatch_records_from_db_count | 5 | 0 | 0.0 |
| mismatch_records_from_json_count | 4 | 0 | 0.0 |
| client_account | 999992 | 12 | 99.99879999039992 |
| transaction_date | 999993 | 11 | 99.99889999229995 |
| transaction_type | 999995 | 9 | 99.99909999549998
| amount | 999995 | 9 | 99.99909999549998 |
| Tolerance percent is 10 % | 0 | 0 | 0.0 |

+ matching_record_count - количество строк, которые совпали в обоих источниках
+ mismatch_records_from_db_count - количество строк, которые есть в БД, но которых нет в json
+ mismatch_records_from_json_count - количество строк, которые есть в json, но которых нет в БД

Далее идут названия колонок, по которым происходила проверка. В последнее строке указано значение толеранса. Описаник колонок:
+ field_name - наименование поля
+ matching_record_count - количество строк, по которым значения совпали
+ mismatch_record_count - количество строк, по которым значения расходятся
+ matching_record_percentage - процентное соотношение между совпавшими и не совпавшими строками

## Затраченное время
| Что делал | Сколько делал (примерно) | Комментарий (если нужно) |
| ------ | ----------- | ------ |
|Выбор инструментов| 30 минут| Задумался насчёт Vertica / ClickHouse, но решил попробовать новую СУБД, да и был хороший, готовый image под рукой|
|Подготовка инфраструктуры| 1 час| Копипастил чужие docker-compose в свой мега файл :)|
|Задание1. Создание и наполнение таблиц|  2 часа| Долго копался с тем, как лучше наполнить таблицы, т.к. внутренний функционал касательно этого не очень большой|
|Задание1. Написание запросов|  1 час| Последние 2 запроса сделал достаточно быстро. С первым было посложнее + думал над тем, как его улучшить|
|Задание1. Оптимизация|  30 минут| Наткнулся на database designer. Решил попробовать |
|Задание2. Выбор источников|  1.5 часа | Если с БД (PostgreSQL) я решил сразу же, то со вторым источником немного призадумался. Думал над реализацией какой-нибудь апишки, но решил, что потрачу на неё слишком много времени. Поэтому решил использовать json. Но, т.к. при работе со спарком желательно читать не с локальной папки, а из какого-то хранилища, то поднял для этого hdfs |
|Задание2. Наполнение данными|  1 час | Сгенерил данные в пг по аналогии с vertica и потом спарком создал нормальный json (т.к. pycharm professional так себе экспортирует json из бд). Потом шаловливыми ручонками поправил json|
|Задание2. Осозание задания|  1 час | Разбирался с тем, что такое вообще реконсиляция и как это делать. Наткнулся на пару статей (https://medium.com/analytics-vidhya/data-reconciliation-in-spark-b185c6a2952b и https://habr.com/ru/post/428443/) которые помогли в этом разобраться|
|Задание2. Написание джобы|  5 часов | Я сначала пытался всё-таки обойтись без hdfs, но что-то у меня так и не получилось нормально это сделать. Решил, что проще загрузить в hdfs. Думал над тем, чтобы сделать анализ динамических датасетов, но на это ушло бы гораздо больше времени|
|Написание документации|  2 часа | Постарался расписать всё максимально подробно, с указанием особенностей, моментов установки, аргументацией при выборе решений|
